{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IC3g9xftV5pCgadlJm0K1smcNzWWAhME","timestamp":1732110779550},{"file_id":"1-eoF_FYsDOCvAJfkfKGE30dKjfrGGHuJ","timestamp":1732096493748},{"file_id":"1fJTQHT5mvtWAbCIZXeUDiUaGRqpkpBEA","timestamp":1732008321482}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Artificial Neural Networks and Deep Learning\n","\n","---\n","\n","## Homework 1: Create new dataset"],"metadata":{"id":"nuwVgG3Vbbka"}},{"cell_type":"markdown","source":["## ðŸŒ Connect Colab to Google Drive"],"metadata":{"id":"dw_-hFm6bjY6"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/DeepL/Homework 1/DeepL Team/Matteo"],"metadata":{"id":"y2S4GWr3Uoa8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## âš™ï¸ Import Libraries"],"metadata":{"id":"d7IqZP5Iblna"}},{"cell_type":"code","source":["%%capture\n","pip install keras-cv"],"metadata":{"id":"vcGrBWvOWMk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CO6_Ft_8T56A"},"outputs":[],"source":["# Set seed for reproducibility\n","seed = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","# Import TensorFlow and Keras\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","\n","# Set seed for TensorFlow\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","\n","# Reduce TensorFlow verbosity\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","# Print TensorFlow version\n","print(tf.__version__)\n","\n","# Import other libraries\n","import requests\n","from io import BytesIO\n","import cv2\n","from PIL import Image\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","\n","from tensorflow.keras import mixed_precision\n","\n","import keras_cv\n","from keras_cv.layers import AugMix, RandAugment\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"]},{"cell_type":"markdown","source":["## â³ Load the Data"],"metadata":{"id":"GN_cpHlSboXV"}},{"cell_type":"code","source":["data=np.load('../training_set.npz') # Load the training data here\n","images=data['images']\n","labels=data['labels']\n","del data # Free memory"],"metadata":{"id":"EiwP1xwFP1NJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Target shape', labels.shape)\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(labels, return_counts=True)\n","print('Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"],"metadata":{"id":"eAhl5mhbxllS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display a sample of images from the dataset, one from each class\n","num_classes = 8 # Number of classes to display\n","class_indices = []\n","\n","# Loop through each class and find one image index for that class\n","for class_label in range(num_classes):\n","    index = np.where(labels == class_label)[0][0] # Find first occurrence of class\n","    class_indices.append(index)\n","\n","# Calculate the number of rows and columns for the subplots\n","num_cols = 4  # Number of images per row\n","num_rows = (num_classes + num_cols - 1) // num_cols # Calculate number of rows needed\n","\n","# Create subplots with specified rows and columns\n","fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 3*num_rows))\n","\n","# Iterate through the selected class indices\n","for i, idx in enumerate(class_indices):\n","    # Calculate row and column index for current image\n","    row = i // num_cols\n","    col = i % num_cols\n","    ax = axes[row, col]  # Access subplot using row and column indices\n","    ax.imshow(np.squeeze(images[idx]), vmin=0., vmax=1.)\n","    ax.set_title(f'Label: {labels[idx][0]}')\n","    ax.axis('off')\n","\n","# Adjust layout and display the images\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"EjJ4BIkZQwmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display a sample of troll images from the dataset\n","troll_indices=[11959, 12159, 12359, 12559, 12759, 12959, 13159, 13359, 13559, 13758]\n","\n","fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n","\n","# Iterate through the selected number of images\n","for i, idx in enumerate(troll_indices):\n","    # Calculate row and column index for current image\n","    row = i // 5\n","    col = i % 5\n","    ax = axes[row, col]  # Access subplot using row and column indices\n","    ax.imshow(np.squeeze(images[idx]), vmin=0., vmax=1.)\n","    ax.set_title(f'Label: {labels[idx][0]}')\n","    ax.axis('off')\n","\n","# Adjust layout and display the images\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"k1Qix9pFSaDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“¤ Outliers"],"metadata":{"id":"1WTJU_CAxP-h"}},{"cell_type":"code","source":["# Deleting obvious outliers (200 for each class)\n","images=images[0:11959]\n","labels=labels[0:11959]"],"metadata":{"id":"y74gTZ7OxSha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Target shape', labels.shape)\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(labels, return_counts=True)\n","print('Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"],"metadata":{"id":"Mewb6L33yGbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","plt.bar(unique, count, color='red')\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Number of Samples\")\n","plt.title(\"Number of Samples per Class\")\n","plt.xticks(unique)  # Ensure all class labels are displayed on the x-axis\n","\n","# Add annotations (number of samples) above each bar\n","for i, v in enumerate(count):\n","    plt.text(unique[i], v + 5, str(v), ha='center', va='bottom')\n","\n","plt.show()"],"metadata":{"id":"N_b4DClUf9Ei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## âž— Split dataset into training and validation"],"metadata":{"id":"tEyvq1lA0enE"}},{"cell_type":"code","source":["training_size = 10000\n","test_size = len(images)-training_size"],"metadata":{"id":"ORVvqrlACs5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train, data_val, labels_train, labels_val = train_test_split(\n","    images,\n","    labels,\n","    test_size=test_size,\n","    random_state=seed,\n","    stratify=labels\n",")\n","\n","# Calculate the unique target labels and their counts for training set\n","unique, count = np.unique(labels_train, return_counts=True)\n","print('Training set Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')\n","\n","# Calculate the unique target labels and their counts for validation set\n","unique, count = np.unique(labels_val, return_counts=True)\n","print('\\nValidation set Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"],"metadata":{"id":"tk9n1mLLEQXg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ” Replicate images to balance under-populated classes in training set"],"metadata":{"id":"G-8cFyKUFBYr"}},{"cell_type":"code","source":["def balance_classes(images, labels, target_size=2000):\n","    \"\"\"Balances classes by replicating images to reach the target size.\"\"\"\n","    unique_labels, counts = np.unique(labels, return_counts=True)\n","    balanced_images = []\n","    balanced_labels = []\n","\n","    for label in unique_labels:\n","        class_indices = np.where(labels == label)[0]\n","        class_images = images[class_indices]\n","        num_to_add = target_size - len(class_images)\n","\n","        if num_to_add > 0:\n","            # Randomly sample with replacement to reach the target\n","            indices_to_replicate = np.random.choice(len(class_images), size=num_to_add, replace=True)\n","            replicated_images = class_images[indices_to_replicate]\n","            balanced_images.extend(list(class_images) + list(replicated_images))\n","            balanced_labels.extend([label] * target_size)\n","        else:\n","            balanced_images.extend(class_images[:target_size])\n","            balanced_labels.extend([label] * target_size)\n","\n","    return np.array(balanced_images), np.array(balanced_labels)\n","\n","\n","# Example usage (assuming you have data_train and labels_train defined)\n","balanced_data_train, balanced_labels_train = balance_classes(data_train, labels_train)\n","\n","del data_train\n","del labels_train\n","\n","# Verify class balance\n","unique, count = np.unique(balanced_labels_train, return_counts=True)\n","print('Balanced Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"],"metadata":{"id":"T7__HthbE_Fs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ–Š Summary"],"metadata":{"id":"2VMo4xmyFvpd"}},{"cell_type":"code","source":["# Convert labels to categorical format using one-hot encoding\n","balanced_labels_train = tfk.utils.to_categorical(balanced_labels_train)\n","labels_val = tfk.utils.to_categorical(labels_val)\n","\n","input_shape = balanced_data_train.shape[1:]\n","print(f'Input shape of the network:\\t {input_shape}')\n","\n","output_shape = balanced_labels_train.shape[1]\n","print(f'Output shape of the network:\\t {output_shape}')\n","\n","# Print the shapes of the resulting sets\n","print('Training set shape:\\t', balanced_data_train.shape, balanced_labels_train.shape)\n","print('Validation set shape:\\t', data_val.shape, labels_val.shape)"],"metadata":{"id":"-pGhMVxJPw7s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ–¼ Apply Augmentation"],"metadata":{"id":"oa43M-9cJAEx"}},{"cell_type":"code","source":["# Initialize AugMix and RandAugment layers from keras_cv\n","augmix = AugMix(\n","    value_range=(0,255),  # Value range for normalization\n","    num_chains=4,         # You can adjust the number of mixed augmentations\n","    severity=0.75,        # Severity of augmentation (range 0-1)\n","    alpha=1.0             # Alpha blending coefficient for mixing\n",")\n","\n","randaugment = RandAugment(\n","    value_range=[0, 255],        # Value range for normalization\n","    augmentations_per_image=4,   # Number of random augmentations to apply\n","    magnitude=0.75                # Magnitude of augmentation (range 0-1)\n",")\n","\n","# Create a function to apply both augmentations\n","def augment_data(X,y):\n","      X = augmix(X)  # Apply AugMix\n","      X = randaugment(X)  # Apply RandAugment()\n","      return X,y\n","\n","\n","# Example usage on a batch of your training data (data_train)\n","augmented_X_train = tf.data.Dataset.from_tensor_slices((balanced_data_train, balanced_labels_train))\n","augmented_X_train = (augmented_X_train.map(lambda x,y: augment_data(x,y), num_parallel_calls=tf.data.AUTOTUNE)\n","                     .batch(32)\n","                     .prefetch(tf.data.AUTOTUNE))\n","\n","for images, labels in augmented_X_train.take(5):\n","    image=images[0]/255.0\n","    image=image.numpy().astype(np.float32)\n","    image=tf.clip_by_value(image, 0, 1)\n","    plt.imshow(image)\n","    plt.show()\n","    print(labels[0])"],"metadata":{"collapsed":true,"id":"5XIU5ezu6gPt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸ“ˆ Create augmented training dataset and validation dataset"],"metadata":{"id":"E5TbrFjGJ643"}},{"cell_type":"code","source":["aug_im=[]\n","aug_lab=[]\n","\n","for images, labels in augmented_X_train:\n","  aug_im.append(images)\n","  aug_lab.append(labels)\n","\n","aug_im=np.concatenate(aug_im)\n","aug_lab=np.concatenate(aug_lab)"],"metadata":{"id":"AsthhLyUKFEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(aug_im.shape)"],"metadata":{"id":"iiQb6ozpLjsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.savez_compressed('../data_augmented_final.npz', images=aug_im, labels=aug_lab)\n","np.savez_compressed('../data_validation_final.npz', images=data_val, labels=labels_val)"],"metadata":{"id":"TjankpMQLnTb"},"execution_count":null,"outputs":[]}]}